





import os
import glob
import numpy as np
import pandas as pd
#import network_fcon as fc
import scipy as sp
from scipy.stats import pearsonr
from scipy.stats import linregress
import seaborn as sns
import matplotlib.pyplot as plt
import re





# Set variables
dataset = 'longglucest_outputmeasures2'
atlas = 'Schaefer2018_1000Parcels_17Networks'
nmaps = ["NMDA", "mGluR5", "GABA"]
maps = ["cest", "NMDA", "mGluR5", "GABA"]

# Set paths
inpath = "/Users/pecsok/Desktop/ImageData/PMACS_remote/data/nmaps/" + dataset
outpath = "/Users/pecsok/Desktop/ImageData/PMACS_remote/nmaps/analysis/" + atlas

# Read in data
cestmat = pd.read_csv(inpath + "/all_subs_GluCEST_" + atlas + "_UNI.csv", sep=',')
NMDAmat = pd.read_csv(inpath + "/all_subs_NMDA_normalized_" + atlas + "_UNI.csv", sep=',')
mGluR5mat = pd.read_csv(inpath + "/all_subs_mGluR5_normalized_" + atlas + "_UNI.csv", sep=',')
GABAmat = pd.read_csv(inpath + "/all_subs_GABA_normalized_" + atlas + "_UNI.csv", sep=',')

cestmat.set_index('Subject', inplace = True)
NMDAmat.set_index('Subject', inplace = True)
mGluR5mat.set_index('Subject', inplace = True)
GABAmat.set_index('Subject', inplace = True)



pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
#print(grp_df)





runCNB = True
runnmaps = True
rundiag = True
rundemo = True
runcest = True
run_grpanalysis = True





# Data Trimming

# Rename columns - remove text before the "_" 
#if "Schaefer" in atlas:
#    cestmat.columns = [col.split('_', 1)[1] if '_' in col else col for col in cestmat.columns]

# Remove parcels with < 20 voxels* 
for i, col in enumerate(cestmat.columns):
    if 'NZcount' in col:
        # Set mean col to nan
        mean_col = cestmat.columns[i - 1]
        sigma_col = cestmat.columns[i + 1]
        cestmat[mean_col] = np.where(cestmat[col] < 20, np.nan, cestmat[mean_col])
        cestmat[sigma_col] = np.where(cestmat[col] < 20, np.nan, cestmat[sigma_col])
        cestmat[col] = np.where(cestmat[col] < 20, np.nan, cestmat[col])
columns = cestmat.columns[cestmat.notnull().sum() > len(cestmat)*.75]

# Apply filter to all maps of interest:
trimmed_cestmat = cestmat[columns]
trimmed_NMDAmat = NMDAmat[columns]
trimmed_GABAmat = GABAmat[columns]
trimmed_mGluR5mat = mGluR5mat[columns]


print(trimmed_cestmat.shape)
#print(trimmed_mGluR5mat)
print(trimmed_NMDAmat.shape)
print(trimmed_GABAmat.shape)


# This cell is the same as above, but loops through the nmap dfs too. See if this gives you dfs all the same size. 

maps = ["cest", "NMDA", "mGluR5", "GABA"]
indfs = [cestmat, NMDAmat, mGluR5mat, GABAmat]
outdfs = {}

for i in range(len(dfs)):
    df = dfs[i]
    nmap = maps[i]
    # Rename columns - remove text before the "_" 
    if "Schaefer" in atlas:
        df.columns = [col.split('_', 1)[1] if '_' in col else col for col in df.columns]
    
    # Remove parcels with < 20 voxels* 
    for j, col in enumerate(df.columns):
        if 'NZcount' in col:
            # Set mean col to nan
            mean_col = df.columns[j - 1]
            sigma_col = df.columns[j + 1]
            df[mean_col] = np.where(df[col] < 20, np.nan, df[mean_col])
            df[sigma_col] = np.where(df[col] < 20, np.nan, df[sigma_col])
            df[col] = np.where(df[col] < 20, np.nan, df[col])
    columns = df.columns[df.notnull().sum() > len(df)*.75]
    filtereddf = df[columns]
    filtereddfs[nmap] = filtereddf
    print(filtereddf.shape)


# Temporary: Remove mysterious zeros in nmap dataframes
dfs = [trimmed_NMDAmat, trimmed_mGluR5mat, trimmed_GABAmat]
for i in range(len(dfs)):
    df = dfs[i]
    df.replace(0, np.nan, inplace=True)


# Save trimmed dfs
trimmed_cestmat.to_csv(outpath + 'trimmed_cestmat' + dataset + atlas + '.csv', index=True)
trimmed_NMDAmat.to_csv(outpath + 'trimmed_NMDAmat' + dataset + atlas + '.csv', index=True)
trimmed_GABAmat.to_csv(outpath + 'trimmed_GABAmat' + dataset + atlas + '.csv', index=True)
trimmed_mGluR5mat.to_csv(outpath + 'trimmed_mGluR5mat' + dataset + atlas + '.csv', index=True)


# Alternative: read in standardized neuromap data
nmapsdf = pd.read_csv("/Users/pecsok/projects/Neuromaps/pecsok_pfns/neuromaps/results/receptor_data_scale1000_17.csv", sep=',')



# Make longform group df
cestmat= trimmed_cestmat
# Get list of parcel names
if "Schaefer" in atlas:
    cestmat.columns = [col.split('_', 1)[1] if '_' in col else col for col in cestmat.columns]
    nmdamat.columns = [col.split('_', 1)[1] if '_' in col else col for col in nmdamat.columns]

parcels = cestmat.filter(like="NZMean").columns.tolist()
parcels = [parcel.replace(' NZMean', '') for parcel in parcels]


# Melt cestmat to get Glu data in long format
cestlong = cestmat.reset_index().melt(id_vars='Subject', value_vars=parcels, 
                                      var_name='Parcel', value_name='Glu_avg')

# Melt nmdamat to get NMDA data in long format
nmda_long = nmdamat.reset_index().melt(id_vars='Subject', value_vars=parcels, 
                                       var_name='Parcel', value_name='NMDA_avg')



# Count the number of non-NaN values per subject per parcel for Glu and NMDA
glu_long['Glu_count'] = ~glu_long['Glu_avg'].isna()
nmda_long['NMDA_count'] = ~nmda_long['NMDA_avg'].isna()

# Merge the long-form dataframes based on Subject and Parcel
long_df = pd.merge(glu_long, nmda_long, on=['Subject', 'Parcel'])

# Convert counts from boolean to actual counts (True=1, False=0)
long_df['Glu_count'] = long_df['Glu_count'].astype(int)
long_df['NMDA_count'] = long_df['NMDA_count'].astype(int)

# Display the long-form dataframe
print(long_df)
"""


# Make group df by diagnosis

    hstatus      parcel    CESTavg    mGluR5      NMDA      GABA        D2
0        NC  NZMean_502   9.828479 -0.377612  0.946837  2.481264 -1.023769
1        NC  NZMean_504   9.437898 -0.633749   0.11543  1.279621 -1.001126

from itertools import repeat 
# Read in nmap data 
#nmapsdf.columns = nmaps 
print(nmapsdf)
nmapsdf.index = range(1,1001)


# Trim cestmat and keep only the columns with avg values.
df = keep(grp_df, (["NZMean", "hstatus"]))
NC_cestNZMeans = df.loc[:, (df.columns.str.contains("NZMean"))]
NCcestavgs = df[df["hstatus"] == "NC"].filter(like="NZMean").mean(axis=0)
PScestavgs = df[df["hstatus"] != "NC"].filter(like="NZMean").mean(axis=0)
parcels = df.filter(like="NZMean").columns.tolist()


cestdf = pd.DataFrame(
    zip(
        list(repeat("NC", len(NCcestavgs))) + list(repeat("PS", len(PScestavgs))),  # Repeat "NC" and "PS"
        parcels + parcels,  # Parcel names repeated for both NC and PS
        np.concatenate([NCcestavgs.values, PScestavgs.values]),
        list(repeat("NaN", len(NCcestavgs))) + list(repeat("NaN", len(NCcestavgs))),
        list(repeat("NaN", len(NCcestavgs))) + list(repeat("NaN", len(NCcestavgs))),
        list(repeat("NaN", len(NCcestavgs))) + list(repeat("NaN", len(NCcestavgs)))
    ),  # Concatenate the values for NC and PS
        #np.concatenate([nmap_NC_parcel_values, nmap_PS_parcel_values])  # Concatenate nmap parcel values  
    columns=["hstatus", "parcel", "CESTavg",'mGluR5', 'NMDA', 'GABA']
)

for nmap in nmaps:
    for i in range(501,1001):
        parcel = "NZMean_" + str(i) 
        if parcel in cestdf["parcel"].values:
            cestdf.loc[cestdf["parcel"] == parcel, nmap] = nmapsdf.loc[i, nmap]            



#print(reho_parcelmat)
reho_parcelmat.to_csv('reho_parcelmat' + fieldstrength + atlas + '.csv', index=True)


#print(fc_parcelmat)
fc_parcelmat.to_csv('fc_parcelmat_' + fieldstrength + atlas + '.csv', index=True)












#BBS2

from sklearn import linear_model
import statsmodels.api as sm
import statsmodels.formula.api as smf
import numpy as np
import matplotlib.pyplot as plt

if run_grpanalysis:
    # Curate data 
    value_counts = grp_df['dx_pscat'].value_counts()
    print(value_counts)
    grp_df['dx_pscat'] = grp_df['dx_pscat'].replace('NC', 'HC')
    grp_df['dx_pscat'] = grp_df['dx_pscat'].replace('PROR', 'PSY')
    grp_df['dx_pscat'] = grp_df['dx_pscat'].replace('PRO', 'PSY')
    grp_df['dx_pscat'] = grp_df['dx_pscat'].replace('S', 'PSY')
    grp_df['dx_pscat'] = grp_df['dx_pscat'].replace('O', 'Other')
    grp_df['dx_pscat'] = grp_df['dx_pscat'].replace('Unknown', 'Other')
    grp_df['dx_pscat'] = grp_df['dx_pscat'].replace('MDD', 'Other')
    value_counts = grp_df['dx_pscat'].value_counts()
    print(value_counts)
    
    colors = pd.DataFrame({'Network': ["Cont", "Default", "DorsAttn", "Vis", "SalVentAttn", "SomMot", "Limbic"],
        'Color': ['PuOr', 'PuRd_r', 'PiYG_r', 'PRGn', 'PiYG', 'GnBu_r', 'terrain_r']}) # 
    
    anova_tables = []
    # Create a scatter plot with a multiple linear regression 
    for network in networks:
        cestcol = "avgCEST_" + network
        # Create a linear regression model for fcon
       # color = colors.loc[colors['Network'] == network, 'Color'].values[0]
       # sns.set_palette(color)

        # Create CNB correlation plot for each network fcon and cest 
        for CNB_score in CNB_scores:
         #   graph_df = grp_df[grp_df['dx_pscat'] != 'Other']
            graph_df = grp_df
            graph_df = graph_df.dropna(subset=[CNB_score, cestcol, network])
            graph_df = graph_df[[CNB_score, cestcol, network]]
            # Define x values and target variable
            X = graph_df[[cestcol, network]]
            Y = graph_df[CNB_score]
            
            ##################################
            # Define formula and model
            #formula = f'{CNB_score} ~ {cestcol} + {network}'
            #model = smf.ols(formula=formula, data=graph_df).fit()
            #print(network + CNB_score)
            #print(model.summary())
            print(graph_df)
            fig = plt.figure()
            ax = fig.add_subplot(111, projection = '3d')
            ax.scatter(graph_df[CNB_score], graph_df[cestcol], graph_df[network])
            ax.set_xlabel(CNB_score)
            ax.set_ylabel(cestcol)
            ax.set_zlabel(network)
            plt.show()
            
            

