import pandas as pd
import os
import glob


# Import CSV with Subject list

# Set path variables. 
datadir = "/Users/pecsok/Desktop/ImageData/PMACS_remote/data/cest/7T_data"

# Import CSV with Subject list
input_csv_path = "~/Desktop/ImageData/PMACS_remote/data/subject_list_031124.csv"
df = pd.read_csv(input_csv_path)
df = df.dropna(subset=['BBLID','SCANID_CEST'])

# Create output df:
output_df = pd.DataFrame(columns=['BBLID', 'Session', 'Data_There'])



# Loop through rows of subjlist
for index, row in df.iterrows():
    bblid = row['BBLID']
    bblid = str(int(bblid))
    session = row['SCANID_CEST']
    session = str(int(session))
    
     # Check if subj data exists
    path = os.path.join(datadir, str(bblid), str(session))
    #print(path)
    data_there = "Yes" if os.path.isdir(path) else "No"
    #print(data_there)
    
    # Add a new row to the output DataFrame
    vals = [bblid, session, data_there]
    #output_df = output_df.append({'BBLID': bblid, 'Session': session, 'Data_There': data_there}, ignore_index=True)
    output_df.loc[len(output_df)] = vals

# Save the output DataFrame to a CSV file
output_csv_path = "output.csv"
output_df.to_csv(output_csv_path, index=False)




missing_cest_071924 = output_df[output_df['Data_There'] == 'No']
print(missing_cest_071924)
print(output_df)


# Bash code to transfer over people that were hiding in old dataset.

bblid = ____
session = ___
# Source 
start=/project/bbl_projects/MotorGluCEST/dicoms

# End point 
finish=/project/bbl_roalf_pecsokphd/projects/glucest-rsfmri/data/cest/7T_data

mkdir $finish/$bblid

cp -r $start/$bblid/$session $finish/$bblid/$session





old_path = "~/Desktop/ImageData/PMACS_remote/data/olddata_summary.csv"
new_path = "/Users/pecsok/Desktop/ImageData/PMACS_remote/data/cest/7T_data"
df_old = pd.read_csv(old_path)
# Remove rows where Directory is the same as Subdirectory
df_old = df_old[df_old['Directory'] != df_old['Subdirectory']]

old_data_status_df = pd.DataFrame(columns=['BBLID', 'Session', 'Duplicated'])


# Loop through df_old and check for duplication
for index, row in df_old.iterrows():
    directory = row['Directory']
    #print(directory)
    subdirectory = row['Subdirectory']
    #print(subdirectory)
    path = os.path.join(new_path, str(directory), str(subdirectory))
    #print(path)
    duplicate = "Yes" if os.path.isdir(path) else "No"
    #print(str(directory), duplicate)
    vals = [directory, subdirectory, duplicate]
    #print(vals)
    old_data_status_df.loc[len(old_data_status_df)] = vals
#print(old_data_status_df)





# Now, transfer over data from old data to a new folder in "data"
extra_data = old_data_status_df[old_data_status_df['Duplicated'] == 'No']
print(extra_data)
extra_data.to_csv("/Users/pecsok/Desktop/ImageData/PMACS_remote/data/cest/extra_data.csv", index=False)

old_path="/project/bbl_projects/MotorGluCEST/dicoms"

mkdir /project/bbl_roalf_pecsokphd/projects/glucest-rsfmri/data/cest/extra_data

# Loop through df_old and check for duplication
for index, row in extra_data.iterrows():
    bblid = row['BBLID']
    #print(directory)
    session = row['Session']

cp -r $old_path/bblid/session 


