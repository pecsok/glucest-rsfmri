





import os
import glob
import numpy as np
import pandas as pd
#import network_fcon as fc
import scipy as sp
from scipy.stats import pearsonr
from scipy.stats import linregress
import seaborn as sns
import matplotlib.pyplot as plt
import re





# Fieldstrength
fieldstrength = '3T'
fc_parcellation = 'Schaefer1017'
cest_parcellation = 'Schaefer2018_1000Parcels_17Networks'
atlas = 'S1000'

# Set paths
inpath = "/Users/pecsok/Desktop/ImageData/PMACS_remote/data/fmri/postprocessed/" + fieldstrength + "/xcp_d"
outpath = "~/Desktop/ImageData/PMACS_remote/analysis/postprocessing/"
clinpath = "~/Desktop/ImageData/PMACS_remote/data/clinical"
cestpath = "/Users/pecsok/Desktop/ImageData/PMACS_remote/data/cest/output_measures/UNI/"

# Choose what to analyse
diag_scores = ["dx_pscat", "hstatus","sex", "age", "race","ethnic"] # ,"axis1_desc1","axis1_stat1"
diag_details = ["axis1_desc1", "axis1_desc2", "axis1_desc3","axis1_desc4","axis1_desc5", "axis1_desc6"]

# Make output dataframes based on metrics of interest
#diag_df = pd.DataFrame(columns = ["BBLID"] + ["Session"] + diag_scores + diag_details)

# Create parcellated output dfs
cestcolumns = []
fccolumns = []
rehocolumns = []
for i in range(501, 1001):
    cestcolumns.append(f'NZMean_{i}')
    cestcolumns.append(f'NZcount_{i}')
    rehocolumns.append(f'reho_{i}')
    fccolumns.append(f'fc_{i}')
cest_parcelmat = pd.DataFrame(columns = ["BBLID"] + ["Session"] + diag_scores + cestcolumns)
fc_parcelmat = pd.DataFrame(columns = ["BBLID","Session"] + diag_scores + [str(i) for i in range(501, 1001)])
reho_parcelmat = pd.DataFrame(columns = ["BBLID"] + ["Session"] + diag_scores + [str(i) for i in range(501, 1001)])
grp_df = pd.DataFrame(columns = ["BBLID"] + ["Session"] + diag_scores + diag_details + cestcolumns + fccolumns + rehocolumns)
grp_df2 = pd.DataFrame(columns = ["BBLID"] + ["Session"] + diag_scores + diag_details + cestcolumns + fccolumns)

print(grp_df.shape)
# Initialize empty lists and vars
bblids = []
sesids = []

# Import group dataframes and set indices
subjlist = pd.read_csv("~/Desktop/ImageData/PMACS_remote/data/pipeline_input_all_subj.csv", sep=',') 
cnbmat = pd.read_csv(clinpath + "/maggie_datarequest_fullcnb-2.csv", sep=',') 
diagmat = pd.read_csv(clinpath + "/maggie_datarequest_diagnosis-2.csv", sep=',')
demomat = pd.read_csv(clinpath + "/maggie_datarequest_demographics-2.csv", sep=',')
# cestmat = pd.read_csv(clinpath + "/demographics.csv", sep='\t') add grp CEST map here

# Reformat some dfs
cnbmat.set_index('bblid', inplace = True)
diagmat.set_index('bblid', inplace = True)
demomat.set_index('bblid', inplace = True)
subjlist = subjlist.dropna(how='all')
subjlist.rename(columns={'fMRI Field Strength': 'fieldstrength'},inplace=True)

# Correct subject list for future! 
subjlist.loc[subjlist['BBLID'] == 88608.0, 'SCANID_rs'] = 'motive2'
subjlist.loc[subjlist['BBLID'] == 20792.0, 'SCANID_rs'] = 'motive2'
subjlist.loc[subjlist['BBLID'] == 88760.0, 'SCANID_rs'] = 'motive2'
subjlist.loc[subjlist['BBLID'] == 112126.0, 'SCANID_rs'] = 'motive1'
subjlist.loc[subjlist['BBLID'] == 91962.0, 'SCANID_rs'] = 'motive1'
subjlist.loc[subjlist['BBLID'] == 89279.0, 'SCANID_rs'] = 'motive1'
subjlist.loc[subjlist['BBLID'] == 91422.0, 'SCANID_rs'] = 'motive1'
subjlist.loc[subjlist['BBLID'] == 121407.0, 'SCANID_rs'] = '9257'
print(subjlist)  



pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
#print(grp_df)






runfcon = True
runreho = True
runCNB = True
rundiag = True
rundemo = True
runcest = True
run_grpanalysis = True





# First, subset subjlist to only include participants with fieldstrength of interest. 
subjlist = subjlist[subjlist['fieldstrength'] == fieldstrength]
# Select BBLID column and loop through all subjs.
subjs = subjlist['BBLID']

for subj in subjs:
    # Set variables and paths:
    bblid = str(int(subj))
    ses=subjlist.loc[subjlist['BBLID'] == float(bblid), 'SCANID_rs'].values[0]
    ses_path = os.path.join(inpath, "sub-" + bblid, "ses-" + ses)
    ids = [bblid, ses]  # Values for the first two columns
    grp_df.loc[len(grp_df)] = ids + [float('nan')] * (len(grp_df.columns) - len(ids))
    #diag_df.loc[len(diag_df)] = ids + [float('nan')] * (len(diag_df.columns) - len(ids))
    fc_parcelmat.loc[len(fc_parcelmat)] = ids + [float('nan')] * (len(fc_parcelmat.columns) - len(ids))
    cest_parcelmat.loc[len(cest_parcelmat)] = ids + [float('nan')] * (len(cest_parcelmat.columns) - len(ids))
    reho_parcelmat.loc[len(reho_parcelmat)] = ids + [float('nan')] * (len(reho_parcelmat.columns) - len(ids))
    #fc_parcelmat.set_index('BBLID', inplace = True)
    #cest_parcelmat.set_index('BBLID', inplace = True)
    
#    if bblid != "102041" and bblid != "20082" and bblid != "88760" and bblid != "20916" and bblid != "22473" and bblid != "132869": 
    ##################################################################################################
    ## CEST
    ##################################################################################################
    if runcest: #88760's CEST output is empty for some reason.
        print("Processing " + bblid + "'s CEST data'")
        # Extract Glu session ID
        gluses = subjlist.loc[subjlist['BBLID'] == float(bblid), 'SCANID_CEST'].values[0].astype(str) #.
        cestid = bblid + "_" + gluses
        print(cestid)
        # Import data
        cest_pattern = cestpath + cestid + "/" + cestid + "-GluCEST-ROI-" + cest_parcellation + "-measures_UNI.tsv"
        print(cest_pattern)
        cestfile = glob.glob(cest_pattern)
        #print(cestfile)
        for file in cestfile:
            if cestfile and os.path.isfile(file):
                cestmat = pd.read_csv(file, sep='\t')
                for i in range(1, 1001):
                    col_name = f'NZMean_{i}'
                    ct_name = f'NZcount_{i}'
                    if col_name in cestmat.columns:
                        mean = cestmat[col_name].values
                        #print(mean[0])
                        count = cestmat[ct_name].values
                        # FIX MAGGIEEEEE Add if statement so we only add CEST value to output dfs if >20 voxels
                        #print(count[0])
                        cest_parcelmat.loc[cest_parcelmat['BBLID'] == bblid, cest_parcelmat.columns == col_name] = mean[0]
                        cest_parcelmat.loc[cest_parcelmat['BBLID'] == bblid, cest_parcelmat.columns == ct_name] = count[0]
                        grp_df.loc[grp_df["BBLID"] == bblid, grp_df.columns == col_name] = mean[0]
                        grp_df.loc[grp_df["BBLID"] == bblid, grp_df.columns == ct_name] = count[0]

    ##################################################################################################
    ## FC
    ##################################################################################################
    if runfcon:
        fcmat_glob = f"{ses_path}/func/*{fc_parcellation}_measure-pearsoncorrelation_conmat.tsv"
        print(fcmat_glob)
        if glob.glob(fcmat_glob) and os.path.isfile(glob.glob(fcmat_glob)[0]): # FIX this is giving error when file doesn't e
            fcmat = pd.read_csv(glob.glob(fcmat_glob)[0], sep='\t') 
            fcmat.set_index('Node', inplace = True)
            
            # Loop through the networks
            print("Running fcon")
            # Select rows and columns corresponding to the network
            np.fill_diagonal(fcmat.values, np.nan)
            fcmat['avg_fc'] = fcmat.mean(axis=1, skipna=True)
            fc_last500 = fcmat['avg_fc'].tail(500).values
            last500 = pd.DataFrame(fc_last500).T
            last500 = last500.rename(columns={i: str(i+1) for i in range(500)})
            fc_parcelmat.iloc[fc_parcelmat['BBLID'] == bblid, 8:] = fc_last500 #fc_parcelmat.index == bblid              
            grp_df.iloc[grp_df['BBLID'] == bblid, 1014:1514] = fc_last500 #fc_parcelmat.index == bblid              
    ##################################################################################################
    ## RH
    ##################################################################################################
    if runreho:
        rehomat_glob = f"{ses_path}/func/*Schaefer1017_desc-reho_timeseries.tsv"
        if glob.glob(rehomat_glob) and os.path.isfile(glob.glob(rehomat_glob)[0]): # FIX this is giving error when file doesn't e
            rehomat = pd.read_csv(glob.glob(rehomat_glob)[0], sep='\t') 
            print(bblid)
            #print(rehomat)
            # Loop through the networks
            print("Running reho")
            # Select rows and columns corresponding to the network
            last500 = rehomat.iloc[:,-500:].values #.tail(500).values
            reho_parcelmat.iloc[reho_parcelmat['BBLID'] == bblid, 8:] = last500 #fc_parcelmat.index == bblid
            grp_df.iloc[grp_df['BBLID'] == bblid, 1514:2014] = last500

    ##################################################################################################
    ## Diagnosis
    ##################################################################################################
    if rundiag:
        # Add hstatus
        for i in range(len(diag_scores)):
            diag_score = diag_scores[i]
            print(diag_score)
            # Select score of interest and add to grp_df
            diagnoses = diagmat[diag_score]
            if int(bblid) in diagnoses.index:
                diagnosis = diagnoses[int(bblid)]
                fc_parcelmat.iloc[fc_parcelmat['BBLID'] == bblid, fc_parcelmat.columns == diag_score ] = diagnosis
                cest_parcelmat.iloc[cest_parcelmat['BBLID'] == bblid, cest_parcelmat.columns == diag_score ] = diagnosis
                grp_df.iloc[grp_df['BBLID'] == bblid, grp_df.columns == diag_score ] = diagnosis
            else:
                diagnosis = "Unknown"
                fc_parcelmat.iloc[fc_parcelmat['BBLID'] == bblid, fc_parcelmat.columns == diag_score ] = diagnosis
                cest_parcelmat.iloc[cest_parcelmat['BBLID'] == bblid, cest_parcelmat.columns == diag_score ] = diagnosis

print("done")


#print(cest_parcelmat)
cest_parcelmat.to_csv('cest_parcelmat' + fieldstrength + atlas + '.csv', index=True)


#print(reho_parcelmat)
reho_parcelmat.to_csv('reho_parcelmat' + fieldstrength + atlas + '.csv', index=True)


#print(fc_parcelmat)
fc_parcelmat.to_csv('fc_parcelmat_' + fieldstrength + atlas + '.csv', index=True)





# Identify parcels that have enough data. Then create parcel df with parcel rows and multimodal columns.
columns = cest_parcelmat.columns[cest_parcelmat.notnull().sum() > len(cest_parcelmat)*.75]
parcels = [col.split('_')[1] for col in columns if '_' in col]
#print(parcels)
unique_list = list(set(parcels))
seen = set()
unique_list = [x for x in parcels if not 
               (x in seen or seen.add(x))]
print(unique_list)


# Filter df:
def column_to_keep(column_name, subj_info, parcels):
    if column_name in subj_info:
        return True
    for num in parcels:
        if num in column_name:
            return True
    return False
subj_info =  ["BBLID", "Session"] + diag_scores    
filtered_grp_cols = [col for col in grp_df.columns if column_to_keep(col, subj_info, parcels)]
filtered_cest_cols = [col for col in cest_parcelmat.columns if column_to_keep(col, subj_info, parcels)]

# Create new DataFrame with only the filtered columns
filtered_grp_df = grp_df[filtered_grp_cols]
filtered_cest_parcelmat = cest_parcelmat[filtered_cest_cols]

# Trim GluCEST values based on whether NZCount is high enough
# @Maggie, fix tomorrow. This is skipping some rows. Figure out why.
numbers = [str(i) for i in range(500,1001)]
for index, row in filtered_cest_parcelmat.iterrows():
    for num in numbers:
        ctname = 'NZcount_' + num
        avgname= 'NZMean_' + num
        if ctname in filtered_cest_parcelmat.columns:
            if row[ctname] < 20:
                #print(ctname)
                #print(filtered_cest_parcelmat.at[index, ctname])
                filtered_cest_parcelmat.at[index, avgname] = np.nan
                filtered_cest_parcelmat.at[index, ctname] = np.nan
                break


print(filtered_cest_parcelmat)
filtered_grp_df.to_csv('filtered_grp_df' + fieldstrength + atlas + '.csv', index=True)
filtered_cest_parcelmat.to_csv('filtered_cest_parcelmat' + fieldstrength + atlas + '.csv', index=True)


#!jupyter nbconvert --to html pipeline.ipynb --output pipeline.html





#BBS2

from sklearn import linear_model
import statsmodels.api as sm
import statsmodels.formula.api as smf
import numpy as np
import matplotlib.pyplot as plt

if run_grpanalysis:
    # Curate data 
    value_counts = grp_df['dx_pscat'].value_counts()
    print(value_counts)
    grp_df['dx_pscat'] = grp_df['dx_pscat'].replace('NC', 'HC')
    grp_df['dx_pscat'] = grp_df['dx_pscat'].replace('PROR', 'PSY')
    grp_df['dx_pscat'] = grp_df['dx_pscat'].replace('PRO', 'PSY')
    grp_df['dx_pscat'] = grp_df['dx_pscat'].replace('S', 'PSY')
    grp_df['dx_pscat'] = grp_df['dx_pscat'].replace('O', 'Other')
    grp_df['dx_pscat'] = grp_df['dx_pscat'].replace('Unknown', 'Other')
    grp_df['dx_pscat'] = grp_df['dx_pscat'].replace('MDD', 'Other')
    value_counts = grp_df['dx_pscat'].value_counts()
    print(value_counts)
    
    colors = pd.DataFrame({'Network': ["Cont", "Default", "DorsAttn", "Vis", "SalVentAttn", "SomMot", "Limbic"],
        'Color': ['PuOr', 'PuRd_r', 'PiYG_r', 'PRGn', 'PiYG', 'GnBu_r', 'terrain_r']}) # 
    
    anova_tables = []
    # Create a scatter plot with a multiple linear regression 
    for network in networks:
        cestcol = "avgCEST_" + network
        # Create a linear regression model for fcon
       # color = colors.loc[colors['Network'] == network, 'Color'].values[0]
       # sns.set_palette(color)

        # Create CNB correlation plot for each network fcon and cest 
        for CNB_score in CNB_scores:
         #   graph_df = grp_df[grp_df['dx_pscat'] != 'Other']
            graph_df = grp_df
            graph_df = graph_df.dropna(subset=[CNB_score, cestcol, network])
            graph_df = graph_df[[CNB_score, cestcol, network]]
            # Define x values and target variable
            X = graph_df[[cestcol, network]]
            Y = graph_df[CNB_score]
            
            ##################################
            # Define formula and model
            #formula = f'{CNB_score} ~ {cestcol} + {network}'
            #model = smf.ols(formula=formula, data=graph_df).fit()
            #print(network + CNB_score)
            #print(model.summary())
            print(graph_df)
            fig = plt.figure()
            ax = fig.add_subplot(111, projection = '3d')
            ax.scatter(graph_df[CNB_score], graph_df[cestcol], graph_df[network])
            ax.set_xlabel(CNB_score)
            ax.set_ylabel(cestcol)
            ax.set_zlabel(network)
            plt.show()
            
            

